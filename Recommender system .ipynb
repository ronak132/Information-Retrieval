{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "\n",
    "The learning objective:\n",
    "to implement, evaluate, and improve upon traditional collaborative filtering recommenders.\n",
    "\n",
    "We will implement collaborative filtering on the subset of Netflix prize dataset, provided sample dataset has only ~2000 items and ~28,000 users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Netflix Data\n",
    "\n",
    "The dataset is subset of movie ratings data from the Netflix Prize Challenge. Download the dataset from Piazza. It contains a train set, test set, movie file, and README file. The last two files are original ones from the Netflix Prize, however; in this homework you will deal with train and test files which both are subsets of the Netflix training data. Each of train and test files has lines having this format: MovieID,UserID,Rating.\n",
    "\n",
    "Our job is to predict a rating in the test set using those provided in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data in test set 100478\n",
      "data in train set 3255352\n"
     ]
    }
   ],
   "source": [
    "dir = 'C:\\\\Users\\\\ronak\\\\netflixdataset\\\\netflix-dataset'\n",
    "lines = 0\n",
    "file = open(dir + '\\\\TestingRatings.txt', 'r') \n",
    "for line in file:\n",
    "    lines=lines+1\n",
    "print('data in test set',lines)\n",
    "\n",
    "lines = 0\n",
    "file = open(dir + '\\\\TrainingRatings.txt', 'r') \n",
    "for line in file:\n",
    "    lines=lines+1\n",
    "print('data in train set',lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Collaborative Filtering\n",
    "\n",
    "In this part, we will implement the basic collaborative filtering algorithm. We consider the first 5,000 users with their associated items in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "colnames=['movId','UserId','Ratings']\n",
    "\n",
    "data = pd.read_csv('C:\\\\Users\\\\ronak\\\\netflixdataset\\\\netflix-dataset\\\\TrainingRatings.txt',names = colnames, header = None) \n",
    "data = data.pivot(index='movId', columns='UserId', values='Ratings')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_mat[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying with corr() function to get matrix directly\n",
    "colnames = ['movId','UserId','actualRatings']\n",
    "test = pd.read_csv('C:\\\\Users\\\\ronak\\\\netflixdataset\\\\netflix-dataset\\\\TestingRatings.txt',names = colnames, header = None)\n",
    "#test = test.pivot(index='UserId', columns='movId', values='actualRatings')\n",
    "#print(test)\n",
    "k = 0.0001\n",
    "#print(test.loc[0,'movId'])\n",
    "for i in test.index.tolist():\n",
    "    summ = 0\n",
    "    mov = test.loc[i,'movId']\n",
    "    #print(mov)\n",
    "    user1 = test.loc[i,'UserId']\n",
    "    #print(user1)\n",
    "    for user2 in data.columns:\n",
    "        #print(data.loc[mov,user2])\n",
    "        #print(data.loc[user2,'mean'])\n",
    "        if not np.isnan(data.loc[mov,user2]):\n",
    "            #print(corr_mat.loc[user1,user2])\n",
    "            corr = corr_mat.loc[user1,user2]\n",
    "            if np.isnan(corr):\n",
    "                corr = 0\n",
    "            summ = summ + corr*(data.loc[mov,user2]-data[user2].mean())\n",
    "    test.loc[i,'predicted'] = data[user1].mean() + k*summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colnames = ['movId','UserId','actualRatings']\n",
    "# test = pd.read_csv('C:\\\\Users\\\\ronak\\\\netflixdataset\\\\netflix-dataset\\\\test_try.txt',names = colnames, header = None)\n",
    "# #test = test.pivot(index='UserId', columns='movId', values='actualRatings')\n",
    "# #print(test)\n",
    "# k = 0.0001\n",
    "# #print(test.loc[0,'movId'])\n",
    "# for i in test.index.tolist():\n",
    "#     summ = 0\n",
    "#     mov = test.loc[i,'movId']\n",
    "#     #print(mov)\n",
    "#     user1 = test.loc[i,'UserId']\n",
    "#     #print(user1)\n",
    "#     for user2 in data.columns:\n",
    "#         #print(data.loc[mov,user2])\n",
    "#         #print(data.loc[user2,'mean'])\n",
    "#         if not np.isnan(data.loc[mov,user2]):\n",
    "#             corr = data[user1].corr(data[user2])\n",
    "#             if np.isnan(corr):\n",
    "#                 corr = 0\n",
    "#             summ = summ + corr*(data.loc[mov,user2]-data[user2].mean())\n",
    "#     test.loc[i,'predicted'] = data[user1].mean() + k*summ\n",
    "        \n",
    "# print (test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluation \n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "colnames = ['no','movId','UserId','actualRatings','predicted']\n",
    "ans = pd.read_csv('C:\\\\Users\\\\ronak\\\\netflixdataset\\\\netflix-dataset\\\\ans_5000.txt',names = colnames,delim_whitespace = True)\n",
    "#print(ans)\n",
    "mse = np.sqrt(mean_squared_error(ans['actualRatings'].values, ans['predicted'].values))\n",
    "mae = np.sqrt(mean_absolute_error(ans['actualRatings'].values, ans['predicted'].values))\n",
    "print('I ran it for first 5000 users on hprc cluster and saved the result in a csv file similar to shown in the next cell and then ')\n",
    "print('loaded that to a df to calculate the error')\n",
    "print('mean square error is ',mse)\n",
    "print('mean absolute error is ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "colnames = ['no','movId','UserId','actualRatings','predicted']\n",
    "ans = pd.read_csv('C:\\\\Users\\\\ronak\\\\netflixdataset\\\\netflix-dataset\\\\results.txt',names = colnames,delim_whitespace = True)\n",
    "#print(ans)\n",
    "mse = np.sqrt(mean_squared_error(ans['actualRatings'].values, ans['predicted'].values))\n",
    "mae = np.sqrt(mean_absolute_error(ans['actualRatings'].values, ans['predicted'].values))\n",
    "print('I ran it for first 10000 users on hprc cluster and saved the result in a csv file similar to shown in the next cell and then ')\n",
    "print('loaded that to a df to calculate the error')\n",
    "print('mean square error is ',mse)\n",
    "print('mean absolute error is ',mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Extensions\n",
    "\n",
    "Given your results in the previous part, can you do better? For this last part you should report on your best attempt at improving MAE and RMSE. Provide code, results, plus a brief discussion on your approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix factorisation\n",
    "'''\n",
    "I have used matrix factorisation to get the results. The number of latent factors(k) were changed from 5 to 75 at interval \n",
    "of 5and the plot was plotted to find the no. of latent factors where the MSE and MAE are least.\n",
    "As it turns out, the number of latent factors which give the minimum MAE and MSE is 15. \n",
    "\n",
    "At a high level, SVD is an algorithm that decomposes a matrix into the best lower rank (i.e. smaller/simpler) approximation \n",
    "of the original matrix. Mathematically, it decomposes into two unitary matrices and a diagonal matrix.\n",
    "\n",
    "R = U Σ Vt\n",
    "\n",
    "where  R is user ratings matrix, U is the user “features” matrix,  Σ is the diagonal matrix of singular values (essentially \n",
    "weights), and Vt is the movie “features” matrix.\n",
    "\n",
    "To get the lower rank approximation, we take these matrices and keep only the top  k features, which we think of as the  \n",
    "most important underlying taste and preference vectors.\n",
    "\n",
    "The mean square error (MSE) is improved with MF to  0.926\n",
    " and mean absolute error (MAE) is improved with MF is  0.854\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "colnames=['movId','UserId','Ratings']\n",
    "#user1 = pd.read_csv('dataset/1.csv', names=colnames, header=None)\n",
    "#data = pd.DataFrame(columns=)\n",
    "data = pd.read_csv('C:\\\\Users\\\\ronak\\\\netflixdataset\\\\netflix-dataset\\\\TrainingRatings.txt',names = colnames, header = None) \n",
    "data = data.pivot(index='UserId', columns='movId', values='Ratings')\n",
    "\n",
    "data = data.apply(lambda x: x.fillna(x.mean()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dict = {}\n",
    "k = 0\n",
    "for i in data.index.values:\n",
    "    #print(i)\n",
    "    if i not in user_dict:\n",
    "        user_dict[i] = k\n",
    "        k = k+1\n",
    "\n",
    "#print(user_dict)\n",
    "\n",
    "user_dict_ = dict((v,k) for k,v in user_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = data.as_matrix()\n",
    "user_ratings_mean = np.mean(R, axis = 1)\n",
    "R_demeaned = R - user_ratings_mean.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "test = pd.read_csv('C:\\\\Users\\\\ronak\\\\netflixdataset\\\\netflix-dataset\\\\try.txt',names = colnames, header = None)\n",
    "mae = []\n",
    "mse = []\n",
    "start = 5\n",
    "end = 75\n",
    "for m in range(start,end,5):\n",
    "    U, sigma, Vt = svds(R_demeaned, k = m)\n",
    "    sigma = np.diag(sigma)\n",
    "\n",
    "    all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)\n",
    "    preds_df = pd.DataFrame(all_user_predicted_ratings, columns = data.columns)\n",
    "    \n",
    "\n",
    "    for i in test.index.tolist():\n",
    "        mov = test.loc[i,'movId']\n",
    "        user = test.loc[i,'UserId']\n",
    "        test.loc[i,'predicted'] = preds_df.loc[user_dict[user],mov]\n",
    "\n",
    "    mse_temp = np.sqrt(mean_squared_error(test['Ratings'].values, test['predicted'].values))\n",
    "    mae_temp = np.sqrt(mean_absolute_error(test['Ratings'].values, test['predicted'].values))\n",
    "\n",
    "    mse.append(mse_temp)\n",
    "    mae.append(mae_temp)\n",
    "    print('mean square error with MF is ',mse_temp)\n",
    "    print('mean absolute error with MF is ',mae_temp)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "k = np.arange(start, end, 5)\n",
    "\n",
    "#plt.plot(range(5,25,5), mse, 'ro')\n",
    "#plt.axis([0, 6, 0, 20])\n",
    "#plt.show()\n",
    "\n",
    "plt.plot(k, mse, 'r--', k, mae, 'bs')\n",
    "\n",
    "plt.xlabel('no. of latent factors')\n",
    "plt.ylabel('MSE and MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.arange(5, 75, 5)\n",
    "\n",
    "#plt.plot(range(5,25,5), mse, 'ro')\n",
    "#plt.axis([0, 6, 0, 20])\n",
    "#plt.show()\n",
    "\n",
    "plt.plot(k, mse, 'r--', k, mae, 'bs')\n",
    "\n",
    "plt.xlabel('no. of latent factors')\n",
    "plt.ylabel('MSE and MAE')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
